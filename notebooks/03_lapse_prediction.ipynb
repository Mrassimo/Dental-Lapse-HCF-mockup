{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # HCF Member Retention Analysis\n",
    "\n",
    " This notebook analyses member retention patterns to help improve member satisfaction and reduce lapse rates.\n",
    "\n",
    " ## Key Insights from Analysis:\n",
    " - Member Base: 200,000 members\n",
    " - Current Retention Rate: 75.1%\n",
    " - Average Premium: $6,368.05\n",
    " - Churn Rate: 24.9% (49,775 members)\n",
    " - Model Performance: AUC-ROC Score: 0.641\n",
    "\n",
    " ## Analysis Goals\n",
    "\n",
    " 1. **Current State**\n",
    "    - What's our retention rate?\n",
    "    - How does it vary by segment?\n",
    "    - What are the trends?\n",
    "\n",
    " 2. **Risk Factors**\n",
    "    - Why do members leave?\n",
    "    - Who is most likely to leave?\n",
    "    - When do they typically leave?\n",
    "\n",
    " 3. **Predictive Modelling**\n",
    "    - Can we predict who will leave?\n",
    "    - How accurate are our predictions?\n",
    "    - What are the key indicators?\n",
    "\n",
    " 4. **Interventions**\n",
    "    - How can we improve retention?\n",
    "    - What interventions work best?\n",
    "    - When should we intervene?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Understanding Features in Data Science\n",
    "\n",
    " In data science, a \"feature\" is any measurable piece of data that can be used to analyse patterns\n",
    " or make predictions. For our member retention analysis, features include:\n",
    "\n",
    " 1. **Numeric Features**\n",
    "    - Claim Amount: How much a member claimed (avg: $1,213.48)\n",
    "    - Premium Amount: How much they pay (median: $5,039.87)\n",
    "    - BMI: Body Mass Index (avg: 27.95)\n",
    "    - Membership Years: How long they've been with HCF\n",
    "\n",
    " 2. **Categorical Features**\n",
    "    - State: Where they live\n",
    "    - Claim Reason: Why they made a claim\n",
    "    - Data Confidentiality Level\n",
    "\n",
    " These features help us understand and predict member behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "# Data manipulation and analysis\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "import matplotlib.pyplot as plt  # For basic plotting\n",
    "import seaborn as sns  # For statistical visualisations\n",
    "import plotly.graph_objects as go  # For interactive plots\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Machine learning and statistical packages\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training/testing sets\n",
    "from sklearn.preprocessing import StandardScaler  # For normalising numeric features\n",
    "from sklearn.metrics import (  # For evaluating model performance\n",
    "    roc_auc_score,  # Area under ROC curve (0.641 achieved)\n",
    "    precision_recall_curve,  # For analyzing precision-recall tradeoff\n",
    "    average_precision_score,  # Average precision (0.350 achieved)\n",
    "    roc_curve  # For plotting ROC curve\n",
    ")\n",
    "import lightgbm as lgb  # Advanced machine learning model (used for churn prediction)\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter  # For survival analysis\n",
    "from scipy import stats  # For statistical tests\n",
    "import plotly.express as px  # For easy plotting\n",
    "from plotly.subplots import make_subplots  # For complex plot layouts\n",
    "\n",
    "# Add project root to Python path for custom imports\n",
    "project_root = str(Path(__file__).parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Set up plotting style for consistent visualisations\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# HCF brand colours for consistent visualisations\n",
    "HCF_COLORS = {\n",
    "    'primary': '#004B87',    # HCF Blue - main brand color\n",
    "    'secondary': '#00A3E0',  # Light Blue - supporting color\n",
    "    'accent': '#FFB81C',     # Gold - for highlighting\n",
    "    'neutral': '#6D6E71'     # Grey - for background elements\n",
    "}\n",
    "\n",
    "# Ensure visualisations directory exists\n",
    "Path(\"visualisations\").mkdir(exist_ok=True)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Data Loading and Feature Engineering\n",
    "\n",
    " Key metrics from the data:\n",
    " - Premium Distribution:\n",
    "   - Median: $5,039.87\n",
    "   - 25th percentile: $3,298.21\n",
    "   - 75th percentile: $8,221.20\n",
    "   - Maximum: $20,696.50\n",
    " - Claims:\n",
    "   - Average claim: $1,213.48\n",
    "   - Claim-to-premium ratio: 19%\n",
    "   - High variation (std: $1,155.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = Path(project_root) / \"data\" / \"processed\" / \"retention_clean.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Feature Engineering\n",
    "# Convert categorical churn to numeric (1 = churned, 0 = retained)\n",
    "df['churned'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Create membership years feature from premium (as a proxy)\n",
    "df['membership_years'] = df['Category Premium'] / 1000\n",
    "\n",
    "# Extract state from address using regex pattern\n",
    "df['state'] = df['Customer_Address'].str.extract(r', ([A-Z]{2}) \\d+')\n",
    "\n",
    "# Basic data exploration\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of members: {len(df):,}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Calculate key business metrics\n",
    "retention_rate = (1 - df['churned'].mean()) * 100  # Percentage of members retained\n",
    "avg_premium = df['Category Premium'].mean()  # Average premium amount\n",
    "risk_segments = df.groupby('state')['churned'].mean() * 100  # Churn rate by state\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Overall Retention Rate: {retention_rate:.1f}%\")\n",
    "print(f\"Average Premium: ${avg_premium:.2f}\")\n",
    "print(\"\\nChurn Risk by State:\")\n",
    "print(risk_segments.round(1))\n",
    "\n",
    "# Show example records\n",
    "print(\"\\nSample Records:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.head())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Survival Analysis\n",
    "\n",
    " Key findings from survival analysis:\n",
    " - Wide range of membership durations observed\n",
    " - Longer-term members show different behavioral patterns\n",
    " - Premium increases need careful management\n",
    " - Members with high claim-to-premium ratios show higher retention\n",
    "\n",
    " We use the Kaplan-Meier estimator to understand:\n",
    " 1. Membership duration patterns\n",
    " 2. Risk factors for early departure\n",
    " 3. Optimal intervention timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_survival_analysis(df, tenure_col='membership_years', event_col='churned', group_col=None):\n",
    "    \"\"\"\n",
    "    Perform survival analysis using the Kaplan-Meier estimator.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The member data\n",
    "    tenure_col : str\n",
    "        Column containing how long members have been with HCF\n",
    "    event_col : str\n",
    "        Column indicating if a member has churned (1) or not (0)\n",
    "    group_col : str, optional\n",
    "        Column to use for grouping (e.g., 'state' or 'Claim Reason')\n",
    "    \"\"\"\n",
    "    # Create base figure for plotting\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # First, calculate overall survival curve\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(\n",
    "        durations=df[tenure_col],     # How long each member has been with HCF\n",
    "        event_observed=df[event_col], # Whether they've churned (1) or not (0)\n",
    "        label='Overall'               # Label for the plot\n",
    "    )\n",
    "    \n",
    "    # Add the overall survival curve to the plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=kmf.timeline,           # Time points\n",
    "            y=kmf.survival_function_.values.flatten(),  # Survival probabilities\n",
    "            mode='lines',\n",
    "            name='Overall Survival',\n",
    "            line=dict(color=HCF_COLORS['primary'], width=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add confidence intervals if available\n",
    "    try:\n",
    "        ci = kmf.confidence_intervals_\n",
    "        if ci is not None:\n",
    "            # Lower confidence bound\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=kmf.timeline,\n",
    "                    y=ci.iloc[:, 0],  # First column is lower bound\n",
    "                    mode='lines',\n",
    "                    line=dict(width=0),\n",
    "                    showlegend=False\n",
    "                )\n",
    "            )\n",
    "            # Upper confidence bound with shading\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=kmf.timeline,\n",
    "                    y=ci.iloc[:, 1],  # Second column is upper bound\n",
    "                    mode='lines',\n",
    "                    fill='tonexty',  # Fill area between upper and lower bounds\n",
    "                    fillcolor=f\"rgba{tuple(list(int(HCF_COLORS['primary'].lstrip('#')[i:i+2], 16) for i in (0, 2, 4)) + [0.2])}\",\n",
    "                    line=dict(width=0),\n",
    "                    showlegend=False\n",
    "                )\n",
    "            )\n",
    "    except (AttributeError, KeyError):\n",
    "        print(\"Note: Confidence intervals not available\")\n",
    "    \n",
    "    # If a grouping column is specified, add survival curves for each group\n",
    "    if group_col and group_col in df.columns:\n",
    "        colors = [HCF_COLORS['secondary'], HCF_COLORS['accent'], HCF_COLORS['neutral']]\n",
    "        for i, group in enumerate(df[group_col].unique()):\n",
    "            mask = df[group_col] == group\n",
    "            if mask.sum() > 0:  # Only plot if we have data for this group\n",
    "                # Calculate survival curve for this group\n",
    "                kmf_group = KaplanMeierFitter()\n",
    "                kmf_group.fit(\n",
    "                    durations=df[mask][tenure_col],\n",
    "                    event_observed=df[mask][event_col],\n",
    "                    label=str(group)\n",
    "                )\n",
    "                \n",
    "                # Add this group's curve to the plot\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=kmf_group.timeline,\n",
    "                        y=kmf_group.survival_function_.values.flatten(),\n",
    "                        mode='lines',\n",
    "                        name=f'{group_col}: {group}',\n",
    "                        line=dict(color=colors[i % len(colors)], width=2)\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    # Customise the plot layout\n",
    "    fig.update_layout(\n",
    "        title='Member Survival Analysis',\n",
    "        xaxis_title='Membership Years (Premium-Based Proxy)',\n",
    "        yaxis_title='Survival Probability',\n",
    "        yaxis=dict(range=[0, 1.05]),  # Probability range 0-1\n",
    "        showlegend=True,\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        hovermode='x unified'  # Show all values at a given x-position\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform overall survival analysis\n",
    "print(\"Analyzing member survival patterns...\")\n",
    "survival_plot = perform_survival_analysis(df)\n",
    "survival_plot.write_html(\"visualisations/survival_analysis.html\")\n",
    "print(\"✓ Overall survival analysis complete\")\n",
    "\n",
    "# Analyze survival by state\n",
    "print(\"\\nAnalyzing survival patterns by state...\")\n",
    "state_survival = perform_survival_analysis(df, group_col='state')\n",
    "state_survival.write_html(\"visualisations/state_survival_analysis.html\")\n",
    "print(\"✓ State-level survival analysis complete\")\n",
    "\n",
    "# Analyze survival by claim reason\n",
    "print(\"\\nAnalyzing survival patterns by claim reason...\")\n",
    "claim_survival = perform_survival_analysis(df, group_col='Claim Reason')\n",
    "claim_survival.write_html(\"visualisations/claim_survival_analysis.html\")\n",
    "print(\"✓ Claim-based survival analysis complete\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Risk Factor Analysis\n",
    "\n",
    " Let's identify the key factors that influence member churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_risk_factors(df, target_col='churned'):\n",
    "    \"\"\"\n",
    "    Analyse and visualise how different features relate to member churn risk.\n",
    "    \n",
    "    This function looks at two types of relationships:\n",
    "    1. Numeric correlations (e.g., how claim amounts relate to churn)\n",
    "    2. Categorical patterns (e.g., churn rates by state)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The member data\n",
    "    target_col : str\n",
    "        The column indicating churn status (1 = churned, 0 = retained)\n",
    "    \"\"\"\n",
    "    # Select relevant numeric features for analysis\n",
    "    numeric_cols = [\n",
    "        'Claim Amount',         # How much was claimed\n",
    "        'Category Premium',     # Member's premium amount\n",
    "        'Premium/Amount Ratio', # Ratio of premium to claims\n",
    "        'BMI',                  # Body Mass Index\n",
    "        'payment_reliability',  # Payment history metric\n",
    "        'premium_increase',     # How much premium has increased\n",
    "        'service_utilisation'   # How much they use services\n",
    "    ]\n",
    "    # Only keep columns that exist in our data\n",
    "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    \n",
    "    # Calculate correlations between numeric features and churn\n",
    "    corr_matrix = df[numeric_cols + [target_col]].corr()[target_col].sort_values(ascending=False)\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig1 = go.Figure(data=go.Heatmap(\n",
    "        z=df[numeric_cols + [target_col]].corr().values,\n",
    "        x=df[numeric_cols + [target_col]].corr().columns,\n",
    "        y=df[numeric_cols + [target_col]].corr().index,\n",
    "        colorscale='RdBu',  # Red-Blue scale: red=positive, blue=negative correlation\n",
    "        zmid=0              # Center the color scale at 0\n",
    "    ))\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        title='Feature Correlation Matrix',\n",
    "        height=800,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    # Analyse categorical features\n",
    "    categorical_cols = [\n",
    "        'Claim Reason',          # Why they made claims\n",
    "        'Data confidentiality',  # Privacy level\n",
    "        'state'                  # Location\n",
    "    ]\n",
    "    \n",
    "    # Create subplots for each categorical feature\n",
    "    fig2 = make_subplots(\n",
    "        rows=len(categorical_cols),\n",
    "        cols=1,\n",
    "        subplot_titles=[f'Churn Rate by {col}' for col in categorical_cols],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # For each categorical feature\n",
    "    for i, col in enumerate(categorical_cols, 1):\n",
    "        if col in df.columns:\n",
    "            # Calculate average churn rate for each category\n",
    "            churn_by_cat = df.groupby(col)['churned'].mean() * 100\n",
    "            \n",
    "            # Add bar chart showing churn rates\n",
    "            fig2.add_trace(\n",
    "                go.Bar(\n",
    "                    x=churn_by_cat.index,\n",
    "                    y=churn_by_cat.values,\n",
    "                    name=col,\n",
    "                    marker_color=HCF_COLORS['primary']\n",
    "                ),\n",
    "                row=i, col=1\n",
    "            )\n",
    "            \n",
    "            # Label the y-axis\n",
    "            fig2.update_yaxes(title_text='Churn Rate (%)', row=i, col=1)\n",
    "    \n",
    "    # Customise the categorical analysis plot\n",
    "    fig2.update_layout(\n",
    "        height=300 * len(categorical_cols),\n",
    "        title_text='Categorical Risk Factor Analysis',\n",
    "        showlegend=False,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Print insights about numeric correlations\n",
    "    print(\"\\nKey Risk Factors:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Numeric Correlations with Churn:\")\n",
    "    for feature, corr in corr_matrix.items():\n",
    "        if feature != target_col:\n",
    "            print(f\"{feature}: {corr:.3f}\")\n",
    "    \n",
    "    return fig1, fig2\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform risk factor analysis\n",
    "print(\"Analysing risk factors...\")\n",
    "correlation_plot, category_plot = analyse_risk_factors(df)\n",
    "correlation_plot.write_html(\"visualisations/correlation_matrix.html\")\n",
    "category_plot.write_html(\"visualisations/categorical_risk_factors.html\")\n",
    "print(\"✓ Risk factor analysis complete\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Predictive Modelling\n",
    "\n",
    " Model Performance:\n",
    " - AUC-ROC Score: 0.641\n",
    " - Average Precision: 0.350\n",
    " - Early stopping at iteration 26\n",
    "\n",
    " Areas for Enhancement:\n",
    " - Additional data points needed:\n",
    "   - Payment reliability\n",
    "   - Service utilization\n",
    "   - Premium increase history\n",
    "\n",
    " Process Overview:\n",
    " 1. **Data Preparation**\n",
    "    - Select numeric features\n",
    "    - Split data into training and testing sets\n",
    "    - Scale features to similar ranges\n",
    "\n",
    " 2. **Model Training**\n",
    "    - Use LightGBM, an advanced machine learning algorithm\n",
    "    - Optimise for AUC (Area Under Curve) metric\n",
    "    - Monitor for overfitting\n",
    "\n",
    " 3. **Model Evaluation**\n",
    "    - ROC Curve: Shows trade-off between true and false positives\n",
    "    - Precision-Recall Curve: Shows model's ability to find churned members\n",
    "    - Calculate overall performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictive_model(df, target_col='churned', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Build and evaluate a machine learning model to predict member churn.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The member data\n",
    "    target_col : str\n",
    "        Column indicating churn status (1 = churned, 0 = retained)\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (0.2 = 20%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : LGBMClassifier\n",
    "        The trained model\n",
    "    roc_fig : plotly Figure\n",
    "        ROC curve visualisation\n",
    "    pr_fig : plotly Figure\n",
    "        Precision-Recall curve visualisation\n",
    "    auc_score : float\n",
    "        Area under ROC curve\n",
    "    avg_precision : float\n",
    "        Average precision score\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare Features\n",
    "    # -----------------------\n",
    "    # Define features that are safe to use (no data leakage)\n",
    "    safe_features = [\n",
    "        'Claim Amount',      # Historical claim amounts\n",
    "        'Category Premium',  # Current premium amount\n",
    "        'BMI'               # Health indicator\n",
    "    ]\n",
    "    \n",
    "    # Select only safe numeric features\n",
    "    feature_cols = [col for col in safe_features if col in df.columns]\n",
    "    print(\"\\nFeatures used in model:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\\n\".join(f\"- {col}\" for col in feature_cols))\n",
    "    \n",
    "    # Create feature matrix X and target vector y\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Add some feature engineering to capture more complex patterns\n",
    "    X['claim_to_premium'] = X['Claim Amount'] / X['Category Premium']\n",
    "    \n",
    "    # Create categorical features using cut instead of qcut for premium\n",
    "    X['premium_bracket'] = pd.cut(\n",
    "        X['Category Premium'],\n",
    "        bins=[0, 1000, 5000, 10000, 15000, float('inf')],\n",
    "        labels=['very_low', 'low', 'medium', 'high', 'very_high']\n",
    "    )\n",
    "    \n",
    "    # Use cut for BMI with domain-specific ranges\n",
    "    X['bmi_bracket'] = pd.cut(\n",
    "        X['BMI'],\n",
    "        bins=[0, 18.5, 25, 30, 35, float('inf')],  # Standard BMI ranges\n",
    "        labels=['underweight', 'normal', 'overweight', 'obese', 'severely_obese']\n",
    "    )\n",
    "    \n",
    "    # Add interaction features\n",
    "    X['high_claim_ratio'] = (X['claim_to_premium'] > X['claim_to_premium'].median()).astype(int)\n",
    "    \n",
    "    # Convert categorical features to numeric\n",
    "    X = pd.get_dummies(X, columns=['premium_bracket', 'bmi_bracket'])\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Print data analysis\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(X.describe().round(2))\n",
    "    \n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    correlations = []\n",
    "    for col in numeric_cols:\n",
    "        corr = X[col].corr(y)\n",
    "        correlations.append((col, abs(corr)))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop 10 Features by Correlation Strength:\")\n",
    "    for col, corr in correlations[:10]:\n",
    "        print(f\"{col}: {corr:.3f}\")\n",
    "    \n",
    "    # Initialise LightGBM classifier with better parameters for synthetic data\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,      # More trees for better learning\n",
    "        learning_rate=0.001,    # Very slow learning rate\n",
    "        num_leaves=4,           # Very conservative tree complexity\n",
    "        min_child_samples=200,  # Require many samples per leaf\n",
    "        subsample=0.6,          # Use 60% of data for each tree\n",
    "        colsample_bytree=0.6,   # Use 60% of features for each tree\n",
    "        reg_alpha=0.5,          # Stronger L1 regularization\n",
    "        reg_lambda=0.5,         # Stronger L2 regularization\n",
    "        random_state=42         # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size,  # Use 20% for testing\n",
    "        random_state=42       # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Scale features to similar ranges\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],  # Validation data\n",
    "        eval_metric='auc',                    # Optimise for AUC\n",
    "        callbacks=[lgb.early_stopping(20)]    # Stop if no improvement for 20 rounds\n",
    "    )\n",
    "    \n",
    "    # Step 3: Make Predictions\n",
    "    # -----------------------\n",
    "    # Get probability of churn for each member\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Step 4: Calculate Performance Metrics\n",
    "    # -----------------------------------\n",
    "    # Area under ROC curve (overall performance)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Precision-Recall curve data\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Step 5: Create ROC Curve Visualisation\n",
    "    # -------------------------------------\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_fig = go.Figure()\n",
    "    \n",
    "    # Add model performance line\n",
    "    roc_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fpr, y=tpr,\n",
    "            mode='lines',\n",
    "            name=f'ROC Curve (AUC = {auc_score:.3f})',\n",
    "            line=dict(color=HCF_COLORS['primary'], width=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add random classifier line\n",
    "    roc_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1], y=[0, 1],\n",
    "            mode='lines',\n",
    "            name='Random Classifier',\n",
    "            line=dict(color='gray', dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Customise ROC plot\n",
    "    roc_fig.update_layout(\n",
    "        title='ROC Curve',\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Step 6: Create Precision-Recall Curve Visualisation\n",
    "    # ------------------------------------------------\n",
    "    pr_fig = go.Figure()\n",
    "    \n",
    "    # Add precision-recall curve\n",
    "    pr_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=recall, y=precision,\n",
    "            mode='lines',\n",
    "            name=f'PR Curve (AP = {avg_precision:.3f})',\n",
    "            line=dict(color=HCF_COLORS['secondary'], width=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Customise PR plot\n",
    "    pr_fig.update_layout(\n",
    "        title='Precision-Recall Curve',\n",
    "        xaxis_title='Recall',\n",
    "        yaxis_title='Precision',\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return model, roc_fig, pr_fig, auc_score, avg_precision\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the predictive model\n",
    "print(\"Building predictive model...\")\n",
    "model, roc_plot, pr_plot, auc_score, ap_score = build_predictive_model(df)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"AUC-ROC Score: {auc_score:.3f}\")  # Higher is better (max 1.0)\n",
    "print(f\"Average Precision: {ap_score:.3f}\")  # Higher is better (max 1.0)\n",
    "\n",
    "# Save visualisations\n",
    "roc_plot.write_html(\"visualisations/roc_curve.html\")\n",
    "pr_plot.write_html(\"visualisations/precision_recall_curve.html\")\n",
    "print(\"\\n✓ Model evaluation complete\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Recommendations\n",
    "\n",
    " Key findings and recommendations based on analysis:\n",
    "\n",
    " 1. **Premium Strategy**\n",
    "    - Review pricing for high-premium categories\n",
    "    - Consider graduated premium increases\n",
    "    - Monitor impact on retention\n",
    "\n",
    " 2. **Claims Management**\n",
    "    - Monitor claim-to-premium ratios (key predictor)\n",
    "    - Develop targeted retention strategies\n",
    "    - Focus on high-claiming members\n",
    "\n",
    " 3. **Risk Mitigation**\n",
    "    - Implement early warning system based on model\n",
    "    - Focus on members with changing claim patterns\n",
    "    - Proactive intervention at critical points\n",
    "\n",
    " 4. **Model Enhancement**\n",
    "    - Collect additional behavioral data\n",
    "    - Regular review of predictions\n",
    "    - Continuous model refinement"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}