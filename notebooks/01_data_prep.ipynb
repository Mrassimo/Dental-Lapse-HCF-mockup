{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # HCF Data Preparation and Cleaning\n",
    "\n",
    " This notebook handles data preparation for the HCF data science project, including:\n",
    " 1. Cleaning and transforming the downloaded data\n",
    " 2. Generating synthetic data to match HCF's context\n",
    "\n",
    " ## Key Metrics:\n",
    " - Member Base: 200,000 members\n",
    " - Average Premium: $6,368.05\n",
    " - Retention Rate: 75.1%\n",
    " - Average Dental Visit: 0.51 visits/year\n",
    " - Average Member Age: 44 years\n",
    "\n",
    " ## Package Overview\n",
    "\n",
    " ### Data Processing\n",
    " - **pandas**: Data manipulation and cleaning\n",
    " - **numpy**: Numerical operations\n",
    " - **sklearn**: Feature scaling and preprocessing\n",
    "\n",
    " ### Data Generation\n",
    " - **faker**: For generating realistic synthetic data\n",
    " - **numpy.random**: For random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker  # For generating realistic synthetic data\n",
    "from sklearn.preprocessing import StandardScaler  # For normalizing features\n",
    "\n",
    "# Set up Faker for Australian data\n",
    "fake = Faker('en_AU')  # Use Australian locale for realistic data\n",
    "\n",
    "# Initialize scaler for numeric features\n",
    "scaler = StandardScaler()  # Will be used to normalize numeric columns\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Clean and Transform Data\n",
    "\n",
    " Let's clean both datasets and transform them to match HCF's Australian context.\n",
    " Key transformations include:\n",
    " - Converting USD to AUD\n",
    " - Mapping regions to Australian states\n",
    " - Creating HCF-specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Basic data cleaning operations.\n",
    "    \n",
    "    Operations performed:\n",
    "    - Handle missing values in numeric columns (using median)\n",
    "    - Handle missing values in text columns (using 'Unknown')\n",
    "    - Ensure data types are appropriate\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The raw data to clean\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The cleaned data\n",
    "    \"\"\"\n",
    "    # Always work with a copy to preserve original data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle missing values in numeric columns using median\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(\n",
    "        df_clean[numeric_cols].median()\n",
    "    )\n",
    "    \n",
    "    # Handle missing values in text columns with 'Unknown'\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    df_clean[categorical_cols] = df_clean[categorical_cols].fillna('Unknown')\n",
    "    \n",
    "    return df_clean\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dates(df, date_cols):\n",
    "    \"\"\"\n",
    "    Convert dates to Australian format and create time-based features.\n",
    "    \n",
    "    Features created:\n",
    "    - Month of each date\n",
    "    - Year of each date\n",
    "    - Quarter of each date\n",
    "    - Australian season\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Data containing date columns\n",
    "    date_cols : list\n",
    "        List of column names containing dates\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Data with additional date-based features\n",
    "    \"\"\"\n",
    "    df_dates = df.copy()\n",
    "    \n",
    "    for col in date_cols:\n",
    "        # Convert to datetime format\n",
    "        df_dates[col] = pd.to_datetime(df_dates[col])\n",
    "        \n",
    "        # Extract basic date components\n",
    "        df_dates[f'{col}_month'] = df_dates[col].dt.month\n",
    "        df_dates[f'{col}_year'] = df_dates[col].dt.year\n",
    "        df_dates[f'{col}_quarter'] = df_dates[col].dt.quarter\n",
    "        \n",
    "        # Map months to Australian seasons\n",
    "        df_dates[f'{col}_season'] = df_dates[col].dt.month.map({\n",
    "            12: 'Summer', 1: 'Summer', 2: 'Summer',  # Australian summer\n",
    "            3: 'Autumn', 4: 'Autumn', 5: 'Autumn',   # Australian autumn\n",
    "            6: 'Winter', 7: 'Winter', 8: 'Winter',   # Australian winter\n",
    "            9: 'Spring', 10: 'Spring', 11: 'Spring'  # Australian spring\n",
    "        })\n",
    "    \n",
    "    return df_dates\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_member_features(df):\n",
    "    \"\"\"\n",
    "    Create HCF-specific member features.\n",
    "    \n",
    "    Features created:\n",
    "    - Age groups (Young Adult, Middle Age, Senior, Elderly)\n",
    "    - High-claimer flag\n",
    "    - State based on postcode\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Member data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Data with additional member features\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Create age groups for segmentation\n",
    "    if 'age' in df_features.columns:\n",
    "        df_features['age_group'] = pd.cut(\n",
    "            df_features['age'],\n",
    "            bins=[0, 30, 50, 70, 100],\n",
    "            labels=['Young Adult', 'Middle Age', 'Senior', 'Elderly']\n",
    "        )\n",
    "    \n",
    "    # Flag high-claiming members (above median)\n",
    "    if 'claims' in df_features.columns:\n",
    "        df_features['high_claimer'] = (\n",
    "            df_features['claims'] > df_features['claims'].median()\n",
    "        )\n",
    "    \n",
    "    # Add state-based features using Australian postcodes\n",
    "    if 'postcode' in df_features.columns:\n",
    "        # Australian postcode ranges by state\n",
    "        state_ranges = {\n",
    "            'NSW': (1000, 2999),  # New South Wales\n",
    "            'ACT': (2600, 2618),  # Australian Capital Territory\n",
    "            'VIC': (3000, 3999),  # Victoria\n",
    "            'QLD': (4000, 4999),  # Queensland\n",
    "            'SA': (5000, 5999),   # South Australia\n",
    "            'WA': (6000, 6999),   # Western Australia\n",
    "            'TAS': (7000, 7999),  # Tasmania\n",
    "            'NT': (800, 999)      # Northern Territory\n",
    "        }\n",
    "        \n",
    "        def get_state(postcode):\n",
    "            \"\"\"Map postcode to Australian state.\"\"\"\n",
    "            try:\n",
    "                postcode = int(postcode)\n",
    "                for state, (start, end) in state_ranges.items():\n",
    "                    if start <= postcode <= end:\n",
    "                        return state\n",
    "                return 'Unknown'\n",
    "            except (ValueError, TypeError):\n",
    "                return 'Unknown'\n",
    "        \n",
    "        df_features['state'] = df_features['postcode'].apply(get_state)\n",
    "    \n",
    "    return df_features\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_health_insurance_data():\n",
    "    \"\"\"\n",
    "    Clean and transform the US health insurance dataset.\n",
    "    Converts to Australian context (e.g., USD to AUD, states to AU states).\n",
    "    \n",
    "    Transformations:\n",
    "    - Convert USD to AUD (rate: 1.5)\n",
    "    - Map US regions to Australian states\n",
    "    - Add member-specific features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Cleaned and transformed insurance data\n",
    "    \"\"\"\n",
    "    # Read the raw insurance data\n",
    "    df = pd.read_csv('data/raw/health_insurance/insurance.csv')\n",
    "    \n",
    "    # Apply basic cleaning operations\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Convert USD to AUD using current rate\n",
    "    aud_rate = 1.5  # Example conversion rate\n",
    "    df['charges'] = df['charges'] * aud_rate\n",
    "    \n",
    "    # Map US regions to Australian states for context\n",
    "    state_mapping = {\n",
    "        'northeast': 'NSW',  # Map US regions to\n",
    "        'northwest': 'VIC',  # approximate Australian\n",
    "        'southeast': 'QLD',  # equivalents based on\n",
    "        'southwest': 'WA'    # relative position\n",
    "    }\n",
    "    \n",
    "    # Add Australian state column\n",
    "    df['state'] = df['region'].map(state_mapping)\n",
    "    \n",
    "    # Add HCF-specific member features\n",
    "    df = create_member_features(df)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.to_csv('data/processed/insurance_clean.csv', index=False)\n",
    "    return df\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_churn_data():\n",
    "    \"\"\"\n",
    "    Clean and transform the insurance customer churn dataset.\n",
    "    Adapts to Australian context and HCF's specific needs.\n",
    "    \n",
    "    Transformations:\n",
    "    - Convert currency values to AUD\n",
    "    - Add member features\n",
    "    - Calculate retention metrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Cleaned and transformed churn data\n",
    "    \"\"\"\n",
    "    # Read the member data\n",
    "    df = pd.read_csv('data/raw/member_data/randomdata.csv')\n",
    "    \n",
    "    # Apply basic cleaning\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    # Convert all currency values to AUD\n",
    "    currency_cols = [col for col in df.columns if 'premium' in col.lower()]\n",
    "    for col in currency_cols:\n",
    "        df[col] = df[col] * 1.5  # Convert to AUD\n",
    "    \n",
    "    # Add HCF-specific member features\n",
    "    df = create_member_features(df)\n",
    "    \n",
    "    # Add retention-specific features\n",
    "    retention_features = {\n",
    "        # Calculate payment reliability (successful/total payments)\n",
    "        'payment_reliability': lambda x: x['successful_payments'] / x['total_payments'] if 'total_payments' in df.columns else None,\n",
    "        # Calculate premium increase percentage\n",
    "        'premium_increase': lambda x: (x['current_premium'] - x['initial_premium']) / x['initial_premium'] if all(col in df.columns for col in ['current_premium', 'initial_premium']) else None,\n",
    "        # Calculate service utilization ratio\n",
    "        'service_utilisation': lambda x: x['services_used'] / x['services_available'] if all(col in df.columns for col in ['services_used', 'services_available']) else None\n",
    "    }\n",
    "    \n",
    "    # Apply retention features where data is available\n",
    "    for name, func in retention_features.items():\n",
    "        try:\n",
    "            df[name] = df.apply(func, axis=1)\n",
    "        except:\n",
    "            print(f\"Couldn't calculate {name} - missing required columns\")\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.to_csv('data/processed/retention_clean.csv', index=False)\n",
    "    return df\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Generate Synthetic Dental Data\n",
    "\n",
    " Create realistic synthetic data for HCF dental centres, including:\n",
    " - Visit patterns (avg 0.51 visits/year)\n",
    " - Treatment types (check-ups most common)\n",
    " - Member demographics (avg age 44)\n",
    " - Geographic distribution (NSW 30.1%, VIC 25%)\n",
    " - Treatment history and costs (avg $256.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dental_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic dental visit data matching HCF's context.\n",
    "    \n",
    "    Data characteristics:\n",
    "    - Visit frequency: 0.51 visits/year average\n",
    "    - Treatment cost: $256.16 average\n",
    "    - Member satisfaction: 8.03/10 average\n",
    "    - Geographic distribution matches population\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of records to generate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Synthetic dental visit data\n",
    "    \"\"\"\n",
    "    # Define Australian states and treatment types\n",
    "    states = ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'ACT']\n",
    "    treatments = ['Check-up', 'Clean', 'Filling', 'Crown', 'Root Canal']\n",
    "    # Define major suburbs for each state\n",
    "    suburbs = {\n",
    "        'NSW': ['Sydney CBD', 'Parramatta', 'Bondi', 'Chatswood', 'Newcastle'],\n",
    "        'VIC': ['Melbourne CBD', 'South Yarra', 'Brunswick', 'Geelong'],\n",
    "        'QLD': ['Brisbane CBD', 'Gold Coast', 'Sunshine Coast'],\n",
    "        'WA': ['Perth CBD', 'Fremantle'],\n",
    "        'SA': ['Adelaide CBD', 'Glenelg'],\n",
    "        'TAS': ['Hobart CBD'],\n",
    "        'NT': ['Darwin CBD'],\n",
    "        'ACT': ['Canberra City']\n",
    "    }\n",
    "    \n",
    "    # Generate base member data\n",
    "    data = {\n",
    "        'member_id': range(1, n_samples + 1),\n",
    "        'age': np.random.normal(45, 15, n_samples).astype(int),  # Age distribution\n",
    "        # State distribution matching population\n",
    "        'state': np.random.choice(states, n_samples, p=[0.3, 0.25, 0.2, 0.1, 0.1, 0.025, 0.0125, 0.0125]),\n",
    "        # Treatment type distribution\n",
    "        'treatment_type': np.random.choice(treatments, n_samples, p=[0.4, 0.3, 0.2, 0.07, 0.03]),\n",
    "        'visit_date': [fake.date_between(start_date='-1y', end_date='today') for _ in range(n_samples)],\n",
    "        'total_cost': np.random.gamma(shape=5, scale=50, size=n_samples),  # Cost distribution\n",
    "        'membership_years': np.random.uniform(1, 10, n_samples).round(1),  # Tenure\n",
    "        'previous_visits': np.random.poisson(lam=2, size=n_samples)  # Visit history\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add suburb based on state\n",
    "    df['suburb'] = df['state'].apply(lambda x: np.random.choice(suburbs[x]))\n",
    "    \n",
    "    # Add dental-specific metrics\n",
    "    df['last_visit_days'] = (datetime.now() - pd.to_datetime(df['visit_date'])).dt.days\n",
    "    df['visit_frequency'] = df['previous_visits'] / df['membership_years']\n",
    "    df['preventive_ratio'] = np.random.uniform(0, 1, n_samples)  # Preventive care ratio\n",
    "    \n",
    "    # Add satisfaction scores (normal distribution around 8.03)\n",
    "    df['satisfaction_score'] = np.random.normal(8, 1, n_samples).clip(0, 10)\n",
    "    \n",
    "    # Save synthetic data\n",
    "    df.to_csv('data/synthetic/dental_visits.csv', index=False)\n",
    "    return df\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_member_data(n_samples=200000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic member data for churn prediction.\n",
    "    \n",
    "    Data characteristics:\n",
    "    - Member base: 200,000\n",
    "    - Retention rate: 75.1%\n",
    "    - Average premium: $6,368.05\n",
    "    - Churn rate: 24.9%\n",
    "    \n",
    "    Features generated with realistic distributions:\n",
    "    - Premium amounts follow market segments\n",
    "    - Claims have relationship with premium but with noise\n",
    "    - BMI follows Australian population distribution\n",
    "    - Churn probability influenced by multiple factors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of members to generate\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Synthetic member data\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    fake = Faker('en_AU')\n",
    "    Faker.seed(random_seed)\n",
    "    \n",
    "    # Generate base member information\n",
    "    data = {\n",
    "        'Customer Name': [fake.name() for _ in range(n_samples)],\n",
    "        'Customer_Address': [fake.address() for _ in range(n_samples)],\n",
    "        'Company Name': [fake.company() for _ in range(n_samples)]\n",
    "    }\n",
    "    \n",
    "    # Define premium tiers based on market research\n",
    "    premium_tiers = {\n",
    "        'Basic': (1200, 2400),    # Basic coverage\n",
    "        'Bronze': (2400, 3600),   # Bronze tier\n",
    "        'Silver': (3600, 5400),   # Silver tier\n",
    "        'Gold': (5400, 8400),     # Gold tier\n",
    "        'Platinum': (8400, 12000) # Platinum tier\n",
    "    }\n",
    "    \n",
    "    # Assign premium tiers with realistic distribution\n",
    "    tier_probabilities = [0.3, 0.25, 0.2, 0.15, 0.1]  # More members in lower tiers\n",
    "    tiers = np.random.choice(list(premium_tiers.keys()), size=n_samples, p=tier_probabilities)\n",
    "    \n",
    "    # Generate premiums within tiers\n",
    "    premiums = []\n",
    "    for tier in tiers:\n",
    "        min_premium, max_premium = premium_tiers[tier]\n",
    "        premium = np.random.uniform(min_premium, max_premium)\n",
    "        # Add realistic noise\n",
    "        premium *= np.random.normal(1, 0.05)  # 5% standard deviation\n",
    "        premiums.append(round(premium, 2))\n",
    "    \n",
    "    data['Category Premium'] = premiums\n",
    "    \n",
    "    # Generate claims based on premium\n",
    "    claim_ratios = np.random.beta(2, 5, n_samples)  # Beta distribution for claim ratios\n",
    "    data['Claim Amount'] = (data['Category Premium'] * claim_ratios * np.random.normal(1, 0.2, n_samples)).round(2)\n",
    "    \n",
    "    # Generate BMI following Australian distribution\n",
    "    bmi_mean = 27.9  # Australian average\n",
    "    bmi_std = 5.5    # Standard deviation\n",
    "    data['BMI'] = np.random.normal(bmi_mean, bmi_std, n_samples)\n",
    "    data['BMI'] = np.clip(data['BMI'], 16, 45).round(1)  # Clip to realistic range\n",
    "    \n",
    "    # Generate claim reasons with realistic distribution\n",
    "    claim_reasons = ['Medical', 'Dental', 'Optical', 'Physio', 'Mental Health']\n",
    "    claim_probabilities = [0.4, 0.25, 0.15, 0.1, 0.1]\n",
    "    data['Claim Reason'] = np.random.choice(claim_reasons, size=n_samples, p=claim_probabilities)\n",
    "    \n",
    "    # Generate data confidentiality preferences\n",
    "    data['Data confidentiality'] = np.random.choice(['Low', 'Medium', 'High'], size=n_samples, p=[0.2, 0.5, 0.3])\n",
    "    \n",
    "    # Generate payment reliability (with some missing values)\n",
    "    data['payment_reliability'] = np.random.beta(8, 2, n_samples)  # Most members pay reliably\n",
    "    mask = np.random.random(n_samples) < 0.1  # 10% missing values\n",
    "    data['payment_reliability'][mask] = np.nan\n",
    "    \n",
    "    # Generate premium increase data (with some missing values)\n",
    "    data['premium_increase'] = np.random.normal(0.05, 0.02, n_samples)  # Mean 5% increase\n",
    "    mask = np.random.random(n_samples) < 0.15  # 15% missing values\n",
    "    data['premium_increase'][mask] = np.nan\n",
    "    \n",
    "    # Generate service utilization (with some missing values)\n",
    "    data['service_utilisation'] = np.random.beta(3, 4, n_samples)  # Right-skewed\n",
    "    mask = np.random.random(n_samples) < 0.12  # 12% missing values\n",
    "    data['service_utilisation'][mask] = np.nan\n",
    "    \n",
    "    # Calculate churn probability based on multiple factors\n",
    "    churn_prob = np.zeros(n_samples)\n",
    "    \n",
    "    # Base churn rate (15%)\n",
    "    churn_prob += 0.15\n",
    "    \n",
    "    # Premium effect (higher premiums slightly increase churn)\n",
    "    premium_effect = (data['Category Premium'] - np.mean(data['Category Premium'])) / np.std(data['Category Premium'])\n",
    "    churn_prob += 0.05 * premium_effect\n",
    "    \n",
    "    # Claim ratio effect (high claims decrease churn)\n",
    "    claim_ratio = data['Claim Amount'] / data['Category Premium']\n",
    "    claim_effect = -0.1 * (claim_ratio - np.mean(claim_ratio)) / np.std(claim_ratio)\n",
    "    churn_prob += claim_effect\n",
    "    \n",
    "    # Payment reliability effect\n",
    "    payment_effect = np.zeros(n_samples)\n",
    "    mask = ~np.isnan(data['payment_reliability'])\n",
    "    payment_effect[mask] = -0.15 * (data['payment_reliability'][mask] - 0.5) * 2\n",
    "    churn_prob += payment_effect\n",
    "    \n",
    "    # Premium increase effect\n",
    "    increase_effect = np.zeros(n_samples)\n",
    "    mask = ~np.isnan(data['premium_increase'])\n",
    "    increase_effect[mask] = 0.2 * (data['premium_increase'][mask] / 0.05)\n",
    "    churn_prob += increase_effect\n",
    "    \n",
    "    # Add random noise\n",
    "    churn_prob += np.random.normal(0, 0.1, n_samples)\n",
    "    \n",
    "    # Clip probabilities to valid range\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "    \n",
    "    # Generate actual churn based on probabilities\n",
    "    data['Churn'] = pd.Series(np.random.random(n_samples) < churn_prob).map({True: 'Yes', False: 'No'})\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add binary churn indicator\n",
    "    df['churned'] = (df['Churn'] == 'Yes').astype(int)\n",
    "    \n",
    "    return df\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for data storage\n",
    "print(\"Setting up directories...\")\n",
    "for directory in ['data/raw/member_data', 'data/processed', 'data/synthetic']:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run the complete data preparation pipeline\n",
    "print(\"\\nStarting data preparation pipeline...\")\n",
    "\n",
    "# 1. Clean and transform insurance data\n",
    "print(\"\\nCleaning health insurance data...\")\n",
    "insurance_df = clean_health_insurance_data()\n",
    "\n",
    "# 2. Clean and transform churn data\n",
    "print(\"\\nCleaning customer churn data...\")\n",
    "churn_df = clean_customer_churn_data()\n",
    "\n",
    "# 3. Generate synthetic dental data\n",
    "print(\"\\nGenerating synthetic dental data...\")\n",
    "dental_df = generate_synthetic_dental_data()\n",
    "\n",
    "# 4. Generate synthetic member data\n",
    "print(\"\\nGenerating synthetic member data...\")\n",
    "member_df = generate_synthetic_member_data()\n",
    "\n",
    "# Save all generated data\n",
    "print(\"\\nSaving data...\")\n",
    "member_df.to_csv('data/raw/member_data/randomdata.csv', index=False)\n",
    "print(\"✓ Member data saved to data/raw/member_data/randomdata.csv\")\n",
    "\n",
    "# Clean the newly generated data\n",
    "print(\"\\nCleaning generated data...\")\n",
    "clean_customer_churn_data()\n",
    "print(\"✓ Generated data cleaned and saved to data/processed/retention_clean.csv\")\n",
    "\n",
    "print(\"\\nData preparation complete!\")\n",
    "print(\"Processed files saved in data/processed/\")\n",
    "print(\"Synthetic data saved in data/synthetic/\") "
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}